{
  "paragraphs": [
    {
      "text": "%md\n\n| ID      | SYSREMMAG             | SYSREMEPOCHS          | SYSREMERR               |\n|---------|:---------------------:|-----------------------|-------------------------|\n| bigint  | \"{1.2, ... 2.2, 3.3}\" | \"{1.2, ... 2.2, 3.3}\" | \"{1.2, ... 2.2, 3.3}\"   |\n",
      "user": "ad\\scox",
      "dateUpdated": "Mar 28, 2017 9:44:30 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eID \u003c/th\u003e\n      \u003cth align\u003d\"center\"\u003eSYSREMMAG \u003c/th\u003e\n      \u003cth\u003eSYSREMEPOCHS \u003c/th\u003e\n      \u003cth\u003eSYSREMERR \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003ebigint \u003c/td\u003e\n      \u003ctd align\u003d\"center\"\u003e\u0026ldquo;{1.2, \u0026hellip; 2.2, 3.3}\u0026rdquo; \u003c/td\u003e\n      \u003ctd\u003e\u0026ldquo;{1.2, \u0026hellip; 2.2, 3.3}\u0026rdquo; \u003c/td\u003e\n      \u003ctd\u003e\u0026ldquo;{1.2, \u0026hellip; 2.2, 3.3}\u0026rdquo; \u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490659714532_720150966",
      "id": "20170327-200834_1009194881",
      "dateCreated": "Mar 27, 2017 8:08:34 PM",
      "dateStarted": "Mar 28, 2017 9:44:30 AM",
      "dateFinished": "Mar 28, 2017 9:44:33 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport numpy as np\n\nclass LightCurve(object):\n    def __init__(self, mag, epochs, err):\n        self.mag \u003d mag\n        self.epocs \u003d epochs\n        self.err \u003d err\n    @staticmethod\n    def load_curves (row):\n        return LightCurve (\n            mag \u003d LightCurve.to_array (row[1]),\n            epochs \u003d LightCurve.to_array (row[2]),\n            err \u003d LightCurve.to_array (row[3]))\n    @staticmethod\n    def to_array (text):\n        a \u003d text.replace (\u0027{\u0027, \u0027\u0027).replace (\u0027}\u0027, \u0027\u0027).split (\u0027,\u0027)\n        return np.array (map (lambda v : float(v), a))",
      "user": "ad\\scox",
      "dateUpdated": "Mar 29, 2017 11:35:49 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1490659334546_-738231256",
      "id": "20170327-200214_193024193",
      "dateCreated": "Mar 27, 2017 8:02:14 PM",
      "dateStarted": "Mar 29, 2017 11:35:50 AM",
      "dateFinished": "Mar 29, 2017 11:35:50 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.types import *\ndef load (input_file, sample_size\u003d1.0):\n    return sqlContext.read.                                     \\\n        format(\u0027com.databricks.spark.csv\u0027).                     \\\n        options(comment\u003d\u0027#\u0027).                                   \\\n        options(delimiter\u003d\",\").                                 \\\n        options(header\u003dTrue).                                   \\\n        load(input_file).rdd.                                   \\\n        sample (False, sample_size, 1234).                      \\\n        map (lambda r : LightCurve.load_curves (r))\n#input_file \u003d \u0027/projects/stars/evryscope/var/data/lcvs.csv\u0027\ninput_file \u003d \"/ssdscratch/scox/evryscope/lcvs.csv\"\ncurves \u003d load (input_file)\n",
      "user": "ad\\scox",
      "dateUpdated": "Mar 29, 2017 11:35:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1490659781141_1045171841",
      "id": "20170327-200941_1006195128",
      "dateCreated": "Mar 27, 2017 8:09:41 PM",
      "dateStarted": "Mar 29, 2017 11:35:51 AM",
      "dateFinished": "Mar 29, 2017 11:35:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ncurves.count ()\n",
      "user": "ad\\scox",
      "dateUpdated": "Mar 29, 2017 11:35:56 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-4651179463938616739.py\", line 346, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-4651179463938616739.py\", line 339, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e\n  File \"/projects/stars/stack/spark/spark2/python/pyspark/rdd.py\", line 1008, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/projects/stars/stack/spark/spark2/python/pyspark/rdd.py\", line 999, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"/projects/stars/stack/spark/spark2/python/pyspark/rdd.py\", line 873, in fold\n    vals \u003d self.mapPartitions(func).collect()\n  File \"/projects/stars/stack/spark/spark2/python/pyspark/rdd.py\", line 776, in collect\n    port \u003d self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/projects/stars/stack/spark/spark2/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/projects/stars/stack/spark/spark2/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/projects/stars/stack/spark/spark2/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 2 cancelled part of cancelled job group zeppelin-20170327-201223_716142911\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:795)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:795)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490659943605_-14041357",
      "id": "20170327-201223_716142911",
      "dateCreated": "Mar 27, 2017 8:12:23 PM",
      "dateStarted": "Mar 29, 2017 11:06:19 AM",
      "dateFinished": "Mar 29, 2017 11:07:29 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ncurves.map (lambda v : np.sum (v.mag)).\\\n    take (10)",
      "user": "ad\\scox",
      "dateUpdated": "Mar 28, 2017 9:59:28 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[3954.8626040877216, 3993.1496547884462, 4088.1283649836591, 3958.0866344876035, 3990.7716485017218, 12800.775073051456, 4021.7451994473777, 3999.4706032869972, 4005.6268526355425, 22293.477833747864]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490708988602_314459080",
      "id": "20170328-094948_836001762",
      "dateCreated": "Mar 28, 2017 9:49:48 AM",
      "dateStarted": "Mar 28, 2017 9:59:28 AM",
      "dateFinished": "Mar 28, 2017 9:59:29 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport sklearn.metrics as met\nimport numpy as np\n\n# Code from http://alexminnaar.com/time-series-classification-and-clustering-with-python.html\n# Implements a simple kNN classifier based on a DTW distance measure\n# Likely that a faster implementation (w/ subsequence search) exists in http://mlpy.sourceforge.net/docs/3.4/dtw.html\n\ndef LB_Keogh(s1, s2, r):\n    \"\"\"Lower-bounded DTW implementation for early-abandon.\n       Should be linear, whereas the full DTW is quadratic. \"\"\" \n    LB_sum \u003d 0\n    for ind, i in enumerate(s1):\n\n        lower_bound\u003dmin(s2[(ind - r if ind - r \u003e\u003d 0 else 0):(ind + r)])\n        upper_bound\u003dmax(s2[(ind - r if ind - r \u003e\u003d 0 else 0):(ind + r)])\n\n        if i \u003e upper_bound:\n            LB_sum \u003d LB_sum + (i - upper_bound)**2\n        elif i \u003c lower_bound:\n            LB_sum \u003d LB_sum + (i - lower_bound)**2\n\n    return np.sqrt(LB_sum)\n    \n    \ndef DTWDistance(s1, s2,w):\n    DTW \u003d {}\n\n    w \u003d max(w, np.abs(len(s1) - len(s2)))\n\n    for i in range(-1, len(s1)):\n        for j in range(-1, len(s2)):\n            DTW[(i, j)] \u003d float(\u0027inf\u0027)\n    DTW[(-1, -1)] \u003d 0\n\n    for i in np.arange(len(s1)):\n        for j in np.arange(np.max(0, i - w), np.min(len(s2), i + w)):\n            dist\u003d (s1[i] - s2[j])**2\n            DTW[(i, j)] \u003d dist + np.min(DTW[(i-1, j)], \n                                        DTW[(i, j-1)], \n                                        DTW[(i-1, j-1)])\n\n    return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n\n\n",
      "user": "ad\\hcorbett",
      "dateUpdated": "Mar 28, 2017 1:15:41 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1490709103791_2118146600",
      "id": "20170328-095143_1888707834",
      "dateCreated": "Mar 28, 2017 9:51:43 AM",
      "dateStarted": "Mar 28, 2017 12:59:42 PM",
      "dateFinished": "Mar 28, 2017 12:59:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "ad\\hcorbett",
      "dateUpdated": "Mar 28, 2017 10:59:53 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1490713193059_-972920083",
      "id": "20170328-105953_1855237260",
      "dateCreated": "Mar 28, 2017 10:59:53 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/evryscope/Curves",
  "id": "2CBY81EDF",
  "angularObjects": {
    "2C8PNVW4G:shared_process": [],
    "2C9M4A84U:shared_process": [],
    "2C8JB5J2A:shared_process": [],
    "2C8UPVAV8:shared_process": [],
    "2C9WWVYVN::2CBY81EDF": [],
    "2CB6QSJQK:shared_process": [],
    "2CB4GRYA4:shared_process": [],
    "2CAZ1XA1G:shared_process": [],
    "2CBGUDB9H:shared_process": [],
    "2C9VT2CHD:shared_process": [],
    "2CBBPS1GQ:shared_process": [],
    "2CAYF7YMG:shared_process": [],
    "2C7NS2RPM:shared_process": [],
    "2CB55MCKF:shared_process": [],
    "2C9P6TDB4:shared_process": [],
    "2C7YD2D51:shared_process": [],
    "2C9UAC7QR:shared_process": [],
    "2C8K1VZ6J:shared_process": [],
    "2CA9JMF94:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}