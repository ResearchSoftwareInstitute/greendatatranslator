{
  "paragraphs": [
    {
      "text": "%md\n\n| ID      | SYSREMMAG             | SYSREMEPOCHS          | SYSREMERR               |\n|---------|:---------------------:|-----------------------|-------------------------|\n| bigint  | \"{1.2, ... 2.2, 3.3}\" | \"{1.2, ... 2.2, 3.3}\" | \"{1.2, ... 2.2, 3.3}\"   |\n",
      "user": "ad\\scox",
      "dateUpdated": "Mar 28, 2017 9:44:30 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eID \u003c/th\u003e\n      \u003cth align\u003d\"center\"\u003eSYSREMMAG \u003c/th\u003e\n      \u003cth\u003eSYSREMEPOCHS \u003c/th\u003e\n      \u003cth\u003eSYSREMERR \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003ebigint \u003c/td\u003e\n      \u003ctd align\u003d\"center\"\u003e\u0026ldquo;{1.2, \u0026hellip; 2.2, 3.3}\u0026rdquo; \u003c/td\u003e\n      \u003ctd\u003e\u0026ldquo;{1.2, \u0026hellip; 2.2, 3.3}\u0026rdquo; \u003c/td\u003e\n      \u003ctd\u003e\u0026ldquo;{1.2, \u0026hellip; 2.2, 3.3}\u0026rdquo; \u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490659714532_720150966",
      "id": "20170327-200834_1009194881",
      "dateCreated": "Mar 27, 2017 8:08:34 PM",
      "dateStarted": "Mar 28, 2017 9:44:30 AM",
      "dateFinished": "Mar 28, 2017 9:44:33 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport numpy as np\n\nclass LightCurve(object):\n    def __init__(self, mag, epochs, err):\n        self.mag \u003d mag\n        self.epocs \u003d epochs\n        self.err \u003d err\n    @staticmethod\n    def load_curves (row):\n        return LightCurve (\n            mag \u003d LightCurve.to_array (row[1]),\n            epochs \u003d LightCurve.to_array (row[2]),\n            err \u003d LightCurve.to_array (row[3]))\n    @staticmethod\n    def to_array (text):\n        a \u003d text.replace (\u0027{\u0027, \u0027\u0027).replace (\u0027}\u0027, \u0027\u0027).split (\u0027,\u0027)\n        return np.array (map (lambda v : float(v), a))",
      "user": "ad\\scox",
      "dateUpdated": "Mar 29, 2017 9:59:21 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": "org.apache.zeppelin.interpreter.InterpreterException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused",
      "apps": [],
      "jobName": "paragraph_1490659334546_-738231256",
      "id": "20170327-200214_193024193",
      "dateCreated": "Mar 27, 2017 8:02:14 PM",
      "dateStarted": "Mar 29, 2017 9:59:21 AM",
      "dateFinished": "Mar 29, 2017 9:59:51 AM",
      "status": "ERROR",
      "errorMessage": "java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:182)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:90)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.init(RemoteInterpreter.java:209)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:375)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.getFormType(LazyOpenInterpreter.java:105)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:365)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:329)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.types import *\ndef load (sample_size\u003d1.0):\n    input_file \u003d \u0027/projects/stars/evryscope/var/data/lcvs.csv\u0027\n    return sqlContext.read.                                     \\\n        format(\u0027com.databricks.spark.csv\u0027).                     \\\n        options(comment\u003d\u0027#\u0027).                                   \\\n        options(delimiter\u003d\",\").                                 \\\n        options(header\u003dTrue).                                   \\\n        load(input_file).rdd.                                   \\\n        sample (False, sample_size, 1234).                      \\\n        map (lambda r : LightCurve.load_curves (r))\ncurves \u003d load ()\n",
      "user": "ad\\scox",
      "dateUpdated": "Mar 29, 2017 9:22:57 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-5619422626720530937.py\", line 346, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-5619422626720530937.py\", line 339, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 12, in \u003cmodule\u003e\n  File \"\u003cstdin\u003e\", line 9, in load\n  File \"/projects/stars/stack/spark/spark2/python/pyspark/sql/readwriter.py\", line 147, in load\n    return self._df(self._jreader.load(path))\n  File \"/projects/stars/stack/spark/spark2/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/projects/stars/stack/spark/spark2/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/projects/stars/stack/spark/spark2/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o63.load.\n: java.io.IOException: No FileSystem for scheme: null\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2584)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2591)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.makeQualifiedPath(SessionCatalog.scala:115)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:145)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.\u003cinit\u003e(SessionCatalog.scala:89)\n\tat org.apache.spark.sql.internal.SessionState.catalog$lzycompute(SessionState.scala:95)\n\tat org.apache.spark.sql.internal.SessionState.catalog(SessionState.scala:95)\n\tat org.apache.spark.sql.internal.SessionState$$anon$1.\u003cinit\u003e(SessionState.scala:112)\n\tat org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:112)\n\tat org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)\n\tat org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:132)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490659781141_1045171841",
      "id": "20170327-200941_1006195128",
      "dateCreated": "Mar 27, 2017 8:09:41 PM",
      "dateStarted": "Mar 29, 2017 9:22:57 AM",
      "dateFinished": "Mar 29, 2017 9:23:00 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ncurves.count ()\n",
      "user": "ad\\scox",
      "dateUpdated": "Mar 28, 2017 9:52:29 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "4400634\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490659943605_-14041357",
      "id": "20170327-201223_716142911",
      "dateCreated": "Mar 27, 2017 8:12:23 PM",
      "dateStarted": "Mar 28, 2017 9:52:29 AM",
      "dateFinished": "Mar 28, 2017 9:59:19 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ncurves.map (lambda v : np.sum (v.mag)).\\\n    take (10)",
      "user": "ad\\scox",
      "dateUpdated": "Mar 28, 2017 9:59:28 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[3954.8626040877216, 3993.1496547884462, 4088.1283649836591, 3958.0866344876035, 3990.7716485017218, 12800.775073051456, 4021.7451994473777, 3999.4706032869972, 4005.6268526355425, 22293.477833747864]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1490708988602_314459080",
      "id": "20170328-094948_836001762",
      "dateCreated": "Mar 28, 2017 9:49:48 AM",
      "dateStarted": "Mar 28, 2017 9:59:28 AM",
      "dateFinished": "Mar 28, 2017 9:59:29 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport sklearn.metrics as met\nimport numpy as np\n\n# Code from http://alexminnaar.com/time-series-classification-and-clustering-with-python.html\n# Implements a simple kNN classifier based on a DTW distance measure\n# Likely that a faster implementation (w/ subsequence search) exists in http://mlpy.sourceforge.net/docs/3.4/dtw.html\n\ndef LB_Keogh(s1, s2, r):\n    \"\"\"Lower-bounded DTW implementation for early-abandon.\n       Should be linear, whereas the full DTW is quadratic. \"\"\" \n    LB_sum \u003d 0\n    for ind, i in enumerate(s1):\n\n        lower_bound\u003dmin(s2[(ind - r if ind - r \u003e\u003d 0 else 0):(ind + r)])\n        upper_bound\u003dmax(s2[(ind - r if ind - r \u003e\u003d 0 else 0):(ind + r)])\n\n        if i \u003e upper_bound:\n            LB_sum \u003d LB_sum + (i - upper_bound)**2\n        elif i \u003c lower_bound:\n            LB_sum \u003d LB_sum + (i - lower_bound)**2\n\n    return np.sqrt(LB_sum)\n    \n    \ndef DTWDistance(s1, s2,w):\n    DTW \u003d {}\n\n    w \u003d max(w, np.abs(len(s1) - len(s2)))\n\n    for i in range(-1, len(s1)):\n        for j in range(-1, len(s2)):\n            DTW[(i, j)] \u003d float(\u0027inf\u0027)\n    DTW[(-1, -1)] \u003d 0\n\n    for i in np.arange(len(s1)):\n        for j in np.arange(np.max(0, i - w), np.min(len(s2), i + w)):\n            dist\u003d (s1[i] - s2[j])**2\n            DTW[(i, j)] \u003d dist + np.min(DTW[(i-1, j)], \n                                        DTW[(i, j-1)], \n                                        DTW[(i-1, j-1)])\n\n    return np.sqrt(DTW[len(s1)-1, len(s2)-1])\n\n\n",
      "user": "ad\\hcorbett",
      "dateUpdated": "Mar 28, 2017 1:15:41 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python"
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1490709103791_2118146600",
      "id": "20170328-095143_1888707834",
      "dateCreated": "Mar 28, 2017 9:51:43 AM",
      "dateStarted": "Mar 28, 2017 12:59:42 PM",
      "dateFinished": "Mar 28, 2017 12:59:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "user": "ad\\hcorbett",
      "dateUpdated": "Mar 28, 2017 10:59:53 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1490713193059_-972920083",
      "id": "20170328-105953_1855237260",
      "dateCreated": "Mar 28, 2017 10:59:53 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/evryscope/Curves",
  "id": "2CBY81EDF",
  "angularObjects": {
    "2C8PNVW4G:shared_process": [],
    "2C9M4A84U:shared_process": [],
    "2C8JB5J2A:shared_process": [],
    "2C8UPVAV8:shared_process": [],
    "2C9WWVYVN::2CBY81EDF": [],
    "2CB6QSJQK:shared_process": [],
    "2CB4GRYA4:shared_process": [],
    "2CAZ1XA1G:shared_process": [],
    "2CBGUDB9H:shared_process": [],
    "2C9VT2CHD:shared_process": [],
    "2CBBPS1GQ:shared_process": [],
    "2CAYF7YMG:shared_process": [],
    "2C7NS2RPM:shared_process": [],
    "2CB55MCKF:shared_process": [],
    "2C9P6TDB4:shared_process": [],
    "2C7YD2D51:shared_process": [],
    "2C9UAC7QR:shared_process": [],
    "2C8K1VZ6J:shared_process": [],
    "2CA9JMF94:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}